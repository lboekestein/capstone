{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping the GDELT dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we need is a df with:\n",
    "- date\n",
    "- source country of article\n",
    "- country article mentions\n",
    "- theme(s) of article OR article text\n",
    "\n",
    "OR\n",
    "- date\n",
    "- percentage of articles from source country x mentioning country y that have theme z\n",
    "    - could be smoothed over time, but not backwards in time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import urllib.parse\n",
    "import urllib.error\n",
    "import pickle\n",
    "import time\n",
    "import sys\n",
    "from IPython.display import clear_output\n",
    "\n",
    "data_files_path = \"../../data/auxiliary_data/\"\n",
    "gdelt_path = \"../../data/gdelt/\"\n",
    "\n",
    "countries_path = f'{data_path}countries_dict.pickle'\n",
    "themes_path = f'{data_path}themes_list.pickle'\n",
    "countries_capitals_path = f'{data_path}countries_capitals.csv'\n",
    "countries_queries_path = f'{data_path}country_queries.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we import the auxiliary datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open themes list\n",
    "with open(themes_path, 'rb') as f:\n",
    "    themes = pickle.load(f)\n",
    "\n",
    "# open the countries and capitals csv\n",
    "countries_capitals = pd.read_csv(countries_capitals_path)\n",
    "# make a countries dictionary with FIPS as key\n",
    "countries = countries_capitals.set_index('FIPS')['Country'].to_dict()\n",
    "\n",
    "# open the queries csv\n",
    "countries_queries = pd.read_csv(countries_queries_path)\n",
    "# make a query dictionary with FIPS as key\n",
    "query_dict = countries_queries.set_index('FIPS')['Query'].to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def querybuilder(dict):\n",
    "    base_url = \"https://api.gdeltproject.org/api/v2/doc/doc?\"\n",
    "    url = base_url + \"&\".join([f\"{key}={value}\" for key, value in dict.items()])\n",
    "    url = urllib.parse.quote(url, safe='():/?&=').replace(\"&theme=\", \"%20theme:\")\n",
    "    return url\n",
    "\n",
    "def get_gdelt_data(theme, country, start_date, end_date, verbose=0):\n",
    "    \n",
    "    if theme == \"ALL\":\n",
    "        dict = {\n",
    "            \"query\": query_dict[country],\n",
    "            \"mode\": \"TimelineSourceCountry\",\n",
    "            \"startdatetime\": start_date,\n",
    "            \"enddatetime\": end_date,\n",
    "            \"format\": \"csv\",\n",
    "            \"timezoom\" : \"yes\",\n",
    "        }\n",
    "    else:\n",
    "        dict = {\n",
    "            \"query\": query_dict[country],\n",
    "            \"theme\": theme,\n",
    "            \"mode\": \"TimelineSourceCountry\",\n",
    "            \"startdatetime\": start_date,\n",
    "            \"enddatetime\": end_date,\n",
    "            \"format\": \"csv\",\n",
    "            \"timezoom\" : \"yes\",\n",
    "        }  \n",
    "        \n",
    "    url = querybuilder(dict)\n",
    "    \n",
    "    if verbose >= 2:\n",
    "        print(url.replace(\"csv\", \"html\"))\n",
    "    try:\n",
    "        df = pd.read_csv(url)\n",
    "        return df\n",
    "    except pd.errors.EmptyDataError:\n",
    "        if verbose >= 2:\n",
    "            print(\"passed\")\n",
    "        pass\n",
    "\n",
    "def format_seconds(seconds):\n",
    "    hours = int(seconds // 3600)\n",
    "    minutes = int((seconds % 3600) // 60)\n",
    "    seconds = seconds % 60\n",
    "    if hours > 0:\n",
    "        return f\"{hours} hours, {minutes} minutes, {round(seconds)} seconds\"\n",
    "    elif minutes > 0:\n",
    "        return f\"{minutes} minutes, {round(seconds)} seconds\"\n",
    "    else:\n",
    "        return f\"{round(seconds, 2)} seconds\"\n",
    "\n",
    "def scrape_gdelt(themes, countries, start_date, end_date, save_int, all=False, verbose=0):\n",
    "    df_list = []\n",
    "    \n",
    "    if all:\n",
    "        themes.append(\"ALL\")\n",
    "\n",
    "    # track time left\n",
    "    total_items = len(themes) * len(countries)\n",
    "    count = 0\n",
    "    api_call_times = []\n",
    "    passed_total = 0\n",
    "    if verbose >= 1:\n",
    "        print(f\"Total queries: {total_items}\")\n",
    "    # set current time\n",
    "    start_time = time.time()\n",
    "                \n",
    "    saved = 0\n",
    "\n",
    "    for theme in themes:\n",
    "        for country in countries:\n",
    "            if verbose >= 2:\n",
    "                print(f\"Scraping {theme} in {country}\")\n",
    "            \n",
    "            # set time when api was called last\n",
    "            last_api_call_time = time.time()\n",
    "\n",
    "            df = get_gdelt_data(theme, country, start_date, end_date, verbose=verbose)\n",
    "            \n",
    "            api_call_times.append(time.time() - last_api_call_time)\n",
    "\n",
    "            if df is not None:\n",
    "                df['theme'] = theme\n",
    "                df['country'] = country\n",
    "                df_list.append(df)\n",
    "            else:\n",
    "                passed_total += 1\n",
    "\n",
    "            count += 1\n",
    "\n",
    "            if count % save_int == 0:\n",
    "                df = pd.concat(df_list)\n",
    "                df.to_csv(f'{gdelt_path}scraped_all/gdelt_data_{count-save_int}_to_{count}.csv')\n",
    "                df_list = []\n",
    "                saved += 1\n",
    "            \n",
    "            elapsed_time = time.time() - start_time\n",
    "            items_per_second = count / elapsed_time\n",
    "            seconds_left = (total_items - count) / items_per_second\n",
    "            \n",
    "            progress_str = f\"Processed {count}/{total_items} queries. {round(items_per_second, 2)} Query/s. Average api time: {round(sum(api_call_times)/len(api_call_times),2)}s. On theme {theme} for {country}                           \\\n",
    "                            \\nElapsed time: {elapsed_time:.2f} seconds. Estimated time left: {format_seconds(seconds_left)}. Saved: {saved}\"\n",
    "            sys.stdout.write('\\x1b[A\\r' + progress_str)\n",
    "            sys.stdout.flush()\n",
    "            \n",
    "            # avoid hitting API rate limit\n",
    "            if time.time() - last_api_call_time < 5:\n",
    "                time.sleep(5 - (time.time() - last_api_call_time))\n",
    "                \n",
    "    df = pd.concat(df_list)\n",
    "    df.to_csv(f'{gdelt_path}scraped_all/gdelt_data_{count-save_int}_to_{count}.csv')\n",
    "    df_list = []\n",
    "    saved += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test for a few countries and a few themes, to see how long it takes per query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 27/27 queries. 0.66 Query/s. Passed: 15. On theme ALL                                                                            \n",
      "Elapsed time: 40.73 seconds. Estimated time left: 0.0 seconds."
     ]
    }
   ],
   "source": [
    "countries_subset = [str(item) for item in countries.keys()]\n",
    "themes_subset = []\n",
    "\n",
    "start_date_test = \"20170101010000\"\n",
    "end_date_test = \"20240301010000\"\n",
    "\n",
    "scarped_test = scrape_gdelt(themes_subset, countries_subset, start_date_test, end_date_test, all=True, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find that it takes approximately 1 second per query. Our final scraping operation will have `236` countries times `4254` themes, which is `1003944` queries. At 0.5 seconds per query this will take approximately `280` hours. Hence we will have to either limit the amount of themes or perform the scraping in chunks.\n",
    "\n",
    "Furthermore, we find that many themes are not present in the dataset. We will have to filter out these themes. For this we will run a scrape operation across all themes with the `TimelineVol` option, to quickly determine which themes actually contain information. This will also help bring down the time it takes to scrape the entire dataset.\n",
    "\n",
    "## Pivoting the table:\n",
    "\n",
    "Nevertheless, we can transform the data we just retrieved into a pivot table to get it into the final format that we need.\n",
    "\n",
    "We rename the columns and clean up the table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Source country</th>\n",
       "      <th>Intensity</th>\n",
       "      <th>Theme</th>\n",
       "      <th>Target country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6077</th>\n",
       "      <td>2017-01-02</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>0.6557</td>\n",
       "      <td>ALL</td>\n",
       "      <td>Austria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4897</th>\n",
       "      <td>2017-01-02</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>0.3279</td>\n",
       "      <td>CRISISLEX_CRISISLEXREC</td>\n",
       "      <td>Austria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>590</th>\n",
       "      <td>2017-01-02</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>LEADER</td>\n",
       "      <td>Austria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1180</th>\n",
       "      <td>2017-01-02</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>USPEC_POLITICS_GENERAL1</td>\n",
       "      <td>Austria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>649</th>\n",
       "      <td>2017-01-02</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>ALL</td>\n",
       "      <td>Azerbaijan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date Source country  Intensity                    Theme  \\\n",
       "6077  2017-01-02    Afghanistan     0.6557                      ALL   \n",
       "4897  2017-01-02    Afghanistan     0.3279   CRISISLEX_CRISISLEXREC   \n",
       "590   2017-01-02    Afghanistan     0.0000                   LEADER   \n",
       "1180  2017-01-02    Afghanistan     0.0000  USPEC_POLITICS_GENERAL1   \n",
       "649   2017-01-02    Afghanistan     0.0000                      ALL   \n",
       "\n",
       "     Target country  \n",
       "6077        Austria  \n",
       "4897        Austria  \n",
       "590         Austria  \n",
       "1180        Austria  \n",
       "649      Azerbaijan  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rename columns\n",
    "scarped_test.columns = [\"Date\", \"Source country\", \"Intensity\", \"Theme\", \"Target country\"]\n",
    "\n",
    "# clean up source country column\n",
    "scarped_test[\"Source country\"] = scarped_test[\"Source country\"].str.replace(\" Volume Intensity\", \"\")\n",
    "\n",
    "# map fip in target country column to country name with countries dictionary\n",
    "scarped_test[\"Target country\"] = scarped_test[\"Target country\"].map(countries)\n",
    "\n",
    "# sort on source country, date and theme\n",
    "scarped_test = scarped_test.sort_values(by=[\"Date\", \"Source country\", \"Target country\", \"Theme\"])\n",
    "\n",
    "scarped_clean = scarped_test[scarped_test[\"Source country\"] != \"\"]\n",
    "\n",
    "scarped_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we pivot the table so that the themes move to the columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Theme</th>\n",
       "      <th>Source country</th>\n",
       "      <th>Target country</th>\n",
       "      <th>ALL</th>\n",
       "      <th>CRISISLEX_CRISISLEXREC</th>\n",
       "      <th>LEADER</th>\n",
       "      <th>USPEC_POLITICS_GENERAL1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-01-02</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>Austria</td>\n",
       "      <td>0.6557</td>\n",
       "      <td>0.3279</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-02</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>Azerbaijan</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-02</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>The Bahamas</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-02</th>\n",
       "      <td>Albania</td>\n",
       "      <td>Austria</td>\n",
       "      <td>1.4085</td>\n",
       "      <td>0.7042</td>\n",
       "      <td>0.3521</td>\n",
       "      <td>0.7042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-02</th>\n",
       "      <td>Albania</td>\n",
       "      <td>Azerbaijan</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Theme      Source country Target country     ALL  CRISISLEX_CRISISLEXREC  \\\n",
       "Date                                                                       \n",
       "2017-01-02    Afghanistan        Austria  0.6557                  0.3279   \n",
       "2017-01-02    Afghanistan     Azerbaijan  0.0000                  0.0000   \n",
       "2017-01-02    Afghanistan    The Bahamas  0.0000                  0.0000   \n",
       "2017-01-02        Albania        Austria  1.4085                  0.7042   \n",
       "2017-01-02        Albania     Azerbaijan  0.0000                  0.0000   \n",
       "\n",
       "Theme       LEADER  USPEC_POLITICS_GENERAL1  \n",
       "Date                                         \n",
       "2017-01-02  0.0000                   0.0000  \n",
       "2017-01-02  0.0000                   0.0000  \n",
       "2017-01-02     NaN                      NaN  \n",
       "2017-01-02  0.3521                   0.7042  \n",
       "2017-01-02  0.0000                   0.0000  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pivot the table\n",
    "scarped_test_pivot = scarped_clean.pivot_table(index=[\"Date\", \"Source country\", \"Target country\"], columns=[\"Theme\"], values=\"Intensity\").reset_index()\n",
    "\n",
    "# set the \"Date\" as the index\n",
    "scarped_test_pivot = scarped_test_pivot.set_index(\"Date\")\n",
    "\n",
    "scarped_test_pivot.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this method we can create a dataframe that contains the volume of coverage in country x talking about country y, and that have theme z. This is the data that we want.\n",
    "\n",
    "First we run the scarpe operation to get the themes that are actually present in the dataset.\n",
    "\n",
    "## Filtering out empty themes\n",
    "\n",
    "To filter out the empty themes, we have to customize the scrape function to scrape the `TimelineVol` option without a query specification. To reduce time we remove all items in the function that we are not interested in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_gdelt_theme(theme, start_date, end_date, verbose=0):\n",
    "    dict = {\n",
    "        \"query\" : \"\",\n",
    "        \"theme\": theme,\n",
    "        \"mode\": \"TimelineVol\",\n",
    "        \"timelinesmooth\" : 0,\n",
    "        \"startdatetime\": start_date,\n",
    "        \"enddatetime\": end_date,\n",
    "        \"format\": \"csv\",\n",
    "        \"timezoom\" : \"yes\",\n",
    "    }        \n",
    "    url = querybuilder(dict).replace(\"theme=\", \"theme:\")\n",
    "    try:\n",
    "        df = pd.read_csv(url)\n",
    "        return True, url\n",
    "    except pd.errors.EmptyDataError:\n",
    "        return False, url\n",
    "    except urllib.error.HTTPError as e:\n",
    "        if e.code == 429:\n",
    "            return \"TIME ERROR\", url\n",
    "    except Exception as e:\n",
    "        return \"ERROR\", e\n",
    "\n",
    "def verify_themes_gdelt(themes, start_date, end_date, verbose=0):\n",
    "    df_list = []\n",
    "\n",
    "    # track time left\n",
    "    total_items = len(themes)\n",
    "    count = 0\n",
    "    passed_total = 0\n",
    "    passed_logs = []\n",
    "\n",
    "    if verbose >= 1:\n",
    "        print(f\"Total queries: {total_items}\")\n",
    "    \n",
    "    # set current time\n",
    "    start_time = time.time()\n",
    "\n",
    "    # time_out time\n",
    "    time_out = 5\n",
    "\n",
    "    crashed = 0\n",
    "    \n",
    "    for theme in themes:\n",
    "        time.sleep(time_out)\n",
    "        passed = False\n",
    "\n",
    "        value, url = check_gdelt_theme(theme, start_date, end_date, verbose=verbose)\n",
    "\n",
    "        if value == \"TIME ERROR\":\n",
    "            print(f\"TOO MANY REQUESTS after {count} queries. Sleeping 10 seconds\")\n",
    "            passed_logs_df = pd.DataFrame(passed_logs, columns=[\"Theme\", \"Passed\", \"URL\"])\n",
    "            passed_logs_df.to_csv(f\"scraped/passed_logs_crashed_recovered_{crashed + 1}.csv\")\n",
    "            time.sleep(60)\n",
    "            time_out += 1\n",
    "            crashed += 1\n",
    "        elif value == \"ERROR\":\n",
    "            print(f\"SOME ERROR, stopped at {theme}. Error message\\n {url}\")\n",
    "            passed_logs_df = pd.DataFrame(passed_logs, columns=[\"Theme\", \"Passed\", \"URL\"])\n",
    "            passed_logs_df.to_csv(f\"scraped/passed_logs_crashed_recovered_{crashed + 1}.csv\")\n",
    "            crashed += 1\n",
    "        elif not value:\n",
    "            passed_total += 1\n",
    "            passed = True\n",
    "\n",
    "        passed_logs.append((theme, passed, url.replace(\"csv\", \"html\")))\n",
    "\n",
    "        # calculate time left\n",
    "        count += 1\n",
    "        elapsed_time = time.time() - start_time\n",
    "        items_per_second = count / elapsed_time\n",
    "        seconds_left = (total_items - count) / items_per_second\n",
    "        \n",
    "        # print time left\n",
    "        progress_str = f\"Processed {count}/{total_items} queries. {round(items_per_second, 2)} Query/s. Passed: {passed_total}.                        \\\n",
    "                        \\nTimeout: {time_out} seconds. Elapsed time: {elapsed_time:.2f} seconds. Estimated time left: {format_seconds(seconds_left)}.\"\n",
    "        sys.stdout.write('\\x1b[A\\r' + progress_str)\n",
    "        sys.stdout.flush()\n",
    "\n",
    "\n",
    "    return passed_logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we go through all themes and check if they are present in the GDELT 2.0 data. I put a 5 second timeout between each query to avoid hitting the rate limit of the API. Since we want to be able to interrupt the process without losing too much data, we will save the data to a file after each batch of 10 themes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = \"20170101010000\"\n",
    "end_date = \"20240301010000\"\n",
    "\n",
    "offset = 1311\n",
    "batch_size = 60\n",
    "\n",
    "while offset < len(themes):\n",
    "    print(f\"Scraping themes {offset} to {offset + batch_size}\\n\")\n",
    "    themes_subset = themes[offset:offset + batch_size]\n",
    "\n",
    "    passed_logs = verify_themes_gdelt(themes_subset, start_date, end_date, verbose=2)\n",
    "\n",
    "    passed_logs_df = pd.DataFrame(passed_logs, columns=[\"Theme\", \"Passed\", \"URL\"])\n",
    "    passed_logs_df.to_csv(f\"{gdelt_path}scraped_all/passed_logs_{offset}_to_{offset+batch_size}.csv\")\n",
    "\n",
    "    offset += batch_size\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Theme</th>\n",
       "      <th>Passed</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TAX_FNCACT</td>\n",
       "      <td>True</td>\n",
       "      <td>https://api.gdeltproject.org/api/v2/doc/doc?qu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TAX_ETHNICITY</td>\n",
       "      <td>True</td>\n",
       "      <td>https://api.gdeltproject.org/api/v2/doc/doc?qu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EPU_POLICY</td>\n",
       "      <td>True</td>\n",
       "      <td>https://api.gdeltproject.org/api/v2/doc/doc?qu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CRISISLEX_CRISISLEXREC</td>\n",
       "      <td>False</td>\n",
       "      <td>https://api.gdeltproject.org/api/v2/doc/doc?qu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TAX_WORLDLANGUAGES</td>\n",
       "      <td>True</td>\n",
       "      <td>https://api.gdeltproject.org/api/v2/doc/doc?qu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Theme  Passed  \\\n",
       "0              TAX_FNCACT    True   \n",
       "1           TAX_ETHNICITY    True   \n",
       "2              EPU_POLICY    True   \n",
       "3  CRISISLEX_CRISISLEXREC   False   \n",
       "4      TAX_WORLDLANGUAGES    True   \n",
       "\n",
       "                                                 URL  \n",
       "0  https://api.gdeltproject.org/api/v2/doc/doc?qu...  \n",
       "1  https://api.gdeltproject.org/api/v2/doc/doc?qu...  \n",
       "2  https://api.gdeltproject.org/api/v2/doc/doc?qu...  \n",
       "3  https://api.gdeltproject.org/api/v2/doc/doc?qu...  \n",
       "4  https://api.gdeltproject.org/api/v2/doc/doc?qu...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert passed_logs to df\n",
    "passed_logs_df = pd.DataFrame(passed_logs, columns=[\"Theme\", \"Passed\", \"URL\"])\n",
    "passed_logs_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now we are ready to perform the scraping operation on all countries and all themes.\n",
    "\n",
    "# Scraping all data [DEMO]\n",
    "\n",
    "First we automate the functions we wrote above to clean and pivot the table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_df(df):\n",
    "    # rename columns\n",
    "    df.columns = [\"Date\", \"Source country\", \"Intensity\", \"Theme\", \"Target country\"]\n",
    "\n",
    "    # clean up source country column\n",
    "    df[\"Source country\"] = df[\"Source country\"].str.replace(\" Volume Intensity\", \"\")\n",
    "\n",
    "    # map fip in target country column to country name with countries dictionary\n",
    "    df[\"Target country\"] = df[\"Target country\"].map(countries)\n",
    "\n",
    "    # sort on source country, date and theme\n",
    "    df = df.sort_values(by=[\"Date\", \"Source country\", \"Target country\", \"Theme\"])\n",
    "\n",
    "    # remove missing source country values\n",
    "    df = df[df[\"Source country\"] != \"\"]\n",
    "\n",
    "    return df\n",
    "\n",
    "def pivot_df(df):\n",
    "    # pivot the table\n",
    "    df_pivot = df.pivot_table(index=[\"Date\", \"Source country\", \"Target country\"], columns=[\"Theme\"], values=\"Intensity\").reset_index()\n",
    "\n",
    "    # set the \"Date\" as the index\n",
    "    df_pivot = df_pivot.set_index(\"Date\")\n",
    "\n",
    "    return df_pivot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we make a function to scrape the data in chunks, and save the chunks in intermediate files. We will make the chunks using the themes, as it is easier to prevent overlap in that way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup parameters\n",
    "countries_final_set = [str(item) for item in countries.keys()]\n",
    "themes_final_set = themes\n",
    "\n",
    "start_date = \"20170101010000\"\n",
    "end_date = \"20240301010000\"\n",
    "\n",
    "chunk_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "offset = 0\n",
    "chunk = themes_final_set[:chunk_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_chunk = scrape_gdelt(chunk, countries_final_set, start_date, end_date, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Theme</th>\n",
       "      <th>Source country</th>\n",
       "      <th>Target country</th>\n",
       "      <th>ALL</th>\n",
       "      <th>CRISISLEX_CRISISLEXREC</th>\n",
       "      <th>LEADER</th>\n",
       "      <th>USPEC_POLITICS_GENERAL1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-01-02</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>Austria</td>\n",
       "      <td>0.6557</td>\n",
       "      <td>0.3279</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-02</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>Azerbaijan</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-02</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>The Bahamas</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-02</th>\n",
       "      <td>Albania</td>\n",
       "      <td>Austria</td>\n",
       "      <td>1.4085</td>\n",
       "      <td>0.7042</td>\n",
       "      <td>0.3521</td>\n",
       "      <td>0.7042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-01-02</th>\n",
       "      <td>Albania</td>\n",
       "      <td>Azerbaijan</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Theme      Source country Target country     ALL  CRISISLEX_CRISISLEXREC  \\\n",
       "Date                                                                       \n",
       "2017-01-02    Afghanistan        Austria  0.6557                  0.3279   \n",
       "2017-01-02    Afghanistan     Azerbaijan  0.0000                  0.0000   \n",
       "2017-01-02    Afghanistan    The Bahamas  0.0000                  0.0000   \n",
       "2017-01-02        Albania        Austria  1.4085                  0.7042   \n",
       "2017-01-02        Albania     Azerbaijan  0.0000                  0.0000   \n",
       "\n",
       "Theme       LEADER  USPEC_POLITICS_GENERAL1  \n",
       "Date                                         \n",
       "2017-01-02  0.0000                   0.0000  \n",
       "2017-01-02  0.0000                   0.0000  \n",
       "2017-01-02     NaN                      NaN  \n",
       "2017-01-02  0.3521                   0.7042  \n",
       "2017-01-02  0.0000                   0.0000  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cl_df = clean_df(scraped_chunk)\n",
    "pivot_df = pivot_df(cl_df)\n",
    "pivot_df.to_csv(f\"{gdelt_path}scraped_all/themes_{offset}_to_{offset+chunk_size}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering out empty themes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "# Get a list of all CSV files in the /scraped_all/ folder\n",
    "csv_files = glob.glob(f'{gdelt_path}/scraped_all/*.csv')\n",
    "\n",
    "# Read in all CSV files and store them in a list\n",
    "dfs = []\n",
    "for file in csv_files:\n",
    "    df = pd.read_csv(file)\n",
    "    dfs.append(df)\n",
    "\n",
    "# Concatenate all dataframes into one\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# make into a dictionary with theme as key and passed as value\n",
    "passed_dict = dict(zip(combined_df[\"Theme\"], combined_df[\"Passed\"]))\n",
    "\n",
    "# filter out items in themed that have True value in passed_dict\n",
    "themes_final_set = [theme for theme in themes[:len(passed_dict)] if not passed_dict[theme]]\n",
    "\n",
    "# saves themes_final_set to pickle\n",
    "with open(f'{gdelt_path}/scarped_all/themes_final_set.pickle', 'wb') as f:\n",
    "    pickle.dump(themes_final_set, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
