{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in country to country coverage intensity for all themes combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Importing libraries, functions and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import urllib.parse\n",
    "import urllib.error\n",
    "import pickle\n",
    "import time\n",
    "import sys\n",
    "import glob\n",
    "from IPython.display import clear_output\n",
    "\n",
    "countries_capitals_path = 'auxilary_data/countries_capitals.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a countries dictionary with FIPS as key\n",
    "countries = countries_capitals.set_index('FIPS')['Country'].to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import the functions from our scraping_gdelt notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def querybuilder(dict):\n",
    "    base_url = \"https://api.gdeltproject.org/api/v2/doc/doc?\"\n",
    "    url = base_url + \"&\".join([f\"{key}={value}\" for key, value in dict.items()])\n",
    "    url = urllib.parse.quote(url, safe='():/?&=').replace(\"&theme=\", \"%20theme:\")\n",
    "    return url\n",
    "\n",
    "def get_gdelt_data(theme, country, start_date, end_date, verbose=0):\n",
    "    \n",
    "    if theme == \"ALL\":\n",
    "        dict = {\n",
    "            \"query\": query_dict[country],\n",
    "            \"mode\": \"TimelineSourceCountry\",\n",
    "            \"startdatetime\": start_date,\n",
    "            \"enddatetime\": end_date,\n",
    "            \"format\": \"csv\",\n",
    "            \"timezoom\" : \"yes\",\n",
    "        }\n",
    "    else:\n",
    "        dict = {\n",
    "            \"query\": query_dict[country],\n",
    "            \"theme\": theme,\n",
    "            \"mode\": \"TimelineSourceCountry\",\n",
    "            \"startdatetime\": start_date,\n",
    "            \"enddatetime\": end_date,\n",
    "            \"format\": \"csv\",\n",
    "            \"timezoom\" : \"yes\",\n",
    "        }  \n",
    "        \n",
    "    url = querybuilder(dict)\n",
    "    \n",
    "    if verbose >= 2:\n",
    "        print(url.replace(\"csv\", \"html\"))\n",
    "    try:\n",
    "        df = pd.read_csv(url)\n",
    "        return df\n",
    "    except pd.errors.EmptyDataError:\n",
    "        if verbose >= 2:\n",
    "            print(\"passed\")\n",
    "        pass\n",
    "\n",
    "def format_seconds(seconds):\n",
    "    hours = int(seconds // 3600)\n",
    "    minutes = int((seconds % 3600) // 60)\n",
    "    seconds = seconds % 60\n",
    "    if hours > 0:\n",
    "        return f\"{hours} hours, {minutes} minutes, {round(seconds)} seconds\"\n",
    "    elif minutes > 0:\n",
    "        return f\"{minutes} minutes, {round(seconds)} seconds\"\n",
    "    else:\n",
    "        return f\"{round(seconds, 2)} seconds\"\n",
    "\n",
    "def scrape_gdelt(themes, countries, start_date, end_date, save_int, all=False, verbose=0):\n",
    "    df_list = []\n",
    "    \n",
    "    if all:\n",
    "        themes.append(\"ALL\")\n",
    "\n",
    "    # track time left\n",
    "    total_items = len(themes) * len(countries)\n",
    "    count = 0\n",
    "    api_call_times = []\n",
    "    passed_total = 0\n",
    "    if verbose >= 1:\n",
    "        print(f\"Total queries: {total_items}\")\n",
    "    # set current time\n",
    "    start_time = time.time()\n",
    "                \n",
    "    saved = 0\n",
    "\n",
    "    for theme in themes:\n",
    "        for country in countries:\n",
    "            if verbose >= 2:\n",
    "                print(f\"Scraping {theme} in {country}\")\n",
    "            \n",
    "            # set time when api was called last\n",
    "            last_api_call_time = time.time()\n",
    "\n",
    "            df = get_gdelt_data(theme, country, start_date, end_date, verbose=verbose)\n",
    "            \n",
    "            api_call_times.append(time.time() - last_api_call_time)\n",
    "\n",
    "            if df is not None:\n",
    "                df['theme'] = theme\n",
    "                df['country'] = country\n",
    "                df_list.append(df)\n",
    "            else:\n",
    "                passed_total += 1\n",
    "\n",
    "            count += 1\n",
    "\n",
    "            if count % save_int == 0:\n",
    "                df = pd.concat(df_list)\n",
    "                df.to_csv(f'scraped_all/gdelt_data_{count-save_int}_to_{count}.csv')\n",
    "                df_list = []\n",
    "                saved += 1\n",
    "            \n",
    "            elapsed_time = time.time() - start_time\n",
    "            items_per_second = count / elapsed_time\n",
    "            seconds_left = (total_items - count) / items_per_second\n",
    "            \n",
    "            progress_str = f\"Processed {count}/{total_items} queries. {round(items_per_second, 2)} Query/s. Average api time: {round(sum(api_call_times)/len(api_call_times),2)}s. On theme {theme} for {country}                           \\\n",
    "                            \\nElapsed time: {elapsed_time:.2f} seconds. Estimated time left: {format_seconds(seconds_left)}. Saved: {saved}\"\n",
    "            sys.stdout.write('\\x1b[A\\r' + progress_str)\n",
    "            sys.stdout.flush()\n",
    "            \n",
    "            # avoid hitting API rate limit\n",
    "            if time.time() - last_api_call_time < 5:\n",
    "                time.sleep(5 - (time.time() - last_api_call_time))\n",
    "                \n",
    "    df = pd.concat(df_list)\n",
    "    df.to_csv(f'scraped_all/gdelt_data_{count-save_int}_to_{count}.csv')\n",
    "    df_list = []\n",
    "    saved += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Scraping the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we set up the parameters for our scraping operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_all = [str(item) for item in countries.keys()]\n",
    "themes_all = []\n",
    "\n",
    "start_date = \"20170101010000\"\n",
    "end_date = \"20240301010000\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we call the scraping function, which saves csv's every 20 queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_gdelt(themes_all, countries_all, start_date, end_date, save_int=20, all=True, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Read the saves csv files back in\n",
    "\n",
    "Since the data is too large for pandas (and our kernel) to handle at once, we have to read in and save the csv in 2 batches. We first make a function to wrangle the data in the desired format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrangle_batch(batch):\n",
    "    if \"Unnamed: 0\" in batch.columns:\n",
    "        batch.drop(columns=[\"Unnamed: 0\"], inplace=True)\n",
    "    batch.columns = [\"Date\", \"Source country\", \"Intensity\", \"Theme\", \"Target country\"]\n",
    "    batch[\"Source country\"] = batch[\"Source country\"].str.replace(\" Volume Intensity\", \"\")\n",
    "    batch[\"Target country\"] = batch[\"Target country\"].map(countries)\n",
    "    batch_pivot = batch.pivot_table(index=[\"Date\", \"Target country\"], columns=[\"Source country\"], values=\"Intensity\").reset_index()\n",
    "    return batch_pivot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then read in all the csv's in folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening df 5 from scraped_all/gdelt_data_80_to_100.csv....\r"
     ]
    }
   ],
   "source": [
    "# for csv file in scraped_all folder\n",
    "csv_files = glob.glob('scraped_all/*.csv')\n",
    "\n",
    "# first batch\n",
    "dataframes = []\n",
    "for i, file in enumerate(csv_files[6:]):\n",
    "    print(f\"Opening df {i} from {file}...\", end=\"\\r\")\n",
    "    df = pd.read_csv(file)\n",
    "    batch = wrangle_batch(df)\n",
    "    dataframes.append(batch)\n",
    "\n",
    "data = pd.concat(dataframes, ignore_index=True)\n",
    "data.to_csv(\"completed/combined_1.csv\", index=False)\n",
    "\n",
    "# second batch\n",
    "dataframes = []\n",
    "for i, file in enumerate(csv_files[6:]):\n",
    "    print(f\"Opening df {i} from {file}...\", end=\"\\r\")\n",
    "    df = pd.read_csv(file)\n",
    "    batch = wrangle_batch(df)\n",
    "    dataframes.append(batch)\n",
    "\n",
    "data = pd.concat(dataframes, ignore_index=True)\n",
    "data.to_csv(\"completed/combined_1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally to combine the two we read them in again, and save them to one combined csv file. We then delete the old csv's to save space (hence they are not in the repository anymore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "part_1 = pd.read_csv(\"completed/combined_1.csv\")\n",
    "part_2 = pd.read_csv(\"completed/combined_2.csv\")\n",
    "\n",
    "combined = pd.concat([part_1, part_2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined.to_csv(\"saved_data/country_to_country_all.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
